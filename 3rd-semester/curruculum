docker:
 - Interacting with services
postgress(psycopg2): from client perspective
 - using SQLAlchemy

1st Task:
 2 articles

End Goal:
	generate a docker compose service from scratch
	use python to generate data
	then, been able to write the data into the the postgress instance.

2nd Task
--------
Google Cloud Storage [GCS]
  - writing functions to perform basic operations.
BigQuery
 - 
 -  write a function to load gcs files into bigquery
 - Task:
   	reading on files format
        - bench mark test: generating a large fake data. then evaluate the perfomance in different file formats
        - expectation: using google colab

Intro to building pipelines with python
----------------------------------------
write simple functions demonstatring how it works with  github actions.


CLoud Fucntions
---------------
batch: gcs, 
event driven: gcs, bq and google cloud popsups
- perform batch etl leverahing cloud functiions, cloud schedular

Github Actiions
----------------
for CI/CD for 

Project Assignemtn
-------
- deploy cloud function through ci/cd
- biult batch pipelines that leverages gcs
- using also schedule querys

1st Case Study (ochestration)
--------------
- Apache Beam || Airflow(an ochesstrator)
- others (prefex[offers pythonic way of writing pipelines], daxter[a worlds of its own], meltano, dbt for transformation, airbyte)

- A Reading Assignment: intro to architecting/designning  etl pipelines


Data Transformation
-------------------
- intro to dbt
- dbt in practice(maybe) 
- take advantage of dbt with google cloud
- models, incremental models, snapshots

- data transformation dbt2

Capstone
--------
- build an end-to-end pipeline
- using prefext


Reading Articles then perform quizess










